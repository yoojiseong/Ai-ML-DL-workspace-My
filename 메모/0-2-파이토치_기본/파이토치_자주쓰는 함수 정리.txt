2-0-PyTorch의 랜덤 텐서 생성 함수 소개 및 예제
2-1-인플레이스(In-Place) 연산이란
2-2-PyTorch view() 함수
2-3-squeeze() vs unsqueeze() 함수 차이점
2-3-2-torch.cat([x.unsqueeze(0), y.unsqueeze(0)], dim=0) 예제
2-4-2-텐서(Tensor) 크기 및 요소 변화 개념 설명
2-4-3-입력 데이터 shape 및 출력 데이터 shape 분석
2-4-4-3차원 텐서(3D Tensor)에서 인덱스 별 위치 이해
2-5-split() vs chunk() 차이점
2-6-torch.index_select() 함수
2-7-torch.cat() (Concatenate) 함수
2-8-torch.stack() 함수
2-9-expand() 예제
2-10-torch.randperm() 예제
2-11-torch.argmax(dim=n) 예제
2-12-torch.topk() 예제
2-13-masked_fill() 예제
2-14-ones(), zeros(), ones_like(), zeros_like() 예제

=============================================================================
2-0-PyTorch의 랜덤 텐서 생성 함수 소개 및 예제

PyTorch에서는 다양한 랜덤(Random) 함수를 제공하며,
주로 무작위 데이터 샘플링, 초기 가중치 설정,
데이터 노이즈 추가 등에 활용됩니다.

1. torch.randn()

✅ 표준 정규 분포(Standard Normal Distribution)에서 랜덤 샘플 생성

평균(𝜇) = 0, 표준편차(𝜎) = 1 인 정규 분포(가우시안 분포)에서 샘플링

입력으로 텐서의 크기(shape)를 지정하면 해당 크기의 랜덤 텐서 생성

import torch

# 3×3 정규 분포 텐서 생성 (평균 0, 표준편차 1)
tensor = torch.randn(3, 3)
print(tensor)

📌 예제 출력

tensor([[ 0.2367, -1.2476,  0.8234],
        [-0.9871,  0.3215, -1.7642],
        [ 0.5923, -0.0032,  0.8493]])


2. torch.rand()

✅ 균등 분포(Uniform Distribution)에서 랜덤 샘플 생성
범위: [0, 1] 사이의 균등한 분포에서 샘플링

tensor = torch.rand(3, 3)  # 0~1 사이 랜덤 값

print(tensor)
📌 예제 출력

tensor([[0.6234, 0.1372, 0.8924],
        [0.2145, 0.9871, 0.4532],
        [0.5623, 0.0014, 0.7143]])


3. torch.randint()

✅ 지정한 정수 범위에서 랜덤 정수 샘플 생성
low (최소값)과 high (최대값)을 지정하여 정수 값을 무작위 생성

tensor = torch.randint(0, 10, (3, 3))  # 0~9 사이 정수 생성

print(tensor)

📌 예제 출력

tensor([[5, 2, 8],
        [1, 7, 3],
        [0, 4, 6]])


4. torch.randint_like()

✅ 기존 텐서와 같은 shape의 랜덤 정수 텐서 생성

x = torch.ones(3, 3)  # 기존 텐서

rand_tensor = torch.randint_like(x, 0, 10)  # 기존 크기 유지, 0~9 정수 샘플링

print(rand_tensor)


5. torch.normal()

✅ 사용자가 지정한 평균(𝜇)과 표준편차(𝜎)로 정규 분포 샘플 생성

mean = torch.tensor([0.0, 2.0, 4.0])  # 평균
std = torch.tensor([1.0, 0.5, 0.1])  # 표준편차

tensor = torch.normal(mean, std)  # 각 요소별로 지정한 평균과 표준편차 적용

print(tensor)

📌 예제 출력

tensor([-0.1032,  2.3213,  4.0734])

6. torch.randperm()

✅ 지정된 범위 내에서 정수를 무작위로 섞음 (순열 생성)

tensor = torch.randperm(10)  # 0~9 정수를 무작위 섞기
print(tensor)

📌 예제 출력

tensor([3, 7, 5, 1, 9, 0, 6, 8, 4, 2])

➡ 데이터를 무작위로 섞을 때 유용

7. torch.bernoulli()

✅ 베르누이 분포(Bernoulli Distribution)에서 샘플링 (0 또는 1)

입력 텐서의 각 요소를 확률로 사용하여 0 또는 1을 반환

probs = torch.tensor([0.2, 0.8, 0.5, 0.9])  # 0~1 사이 확률값

tensor = torch.bernoulli(probs)  # 각 확률에 따라 0 또는 1 생성

print(tensor)

📌 예제 출력

tensor([0., 1., 0., 1.])


8. torch.multinomial()

✅ 다항 분포(Multinomial Distribution)에서 샘플링

probs 텐서가 주어졌을 때, num_samples 개수만큼 랜덤 샘플을 추출

probs = torch.tensor([0.1, 0.2, 0.3, 0.4])  # 각 요소의 선택 확률
tensor = torch.multinomial(probs, 2, replacement=True)  # 2개 선택 (복원 추출)
print(tensor)

📌 예제 출력

tensor([3, 1])


9. torch.seed()

✅ 랜덤 시드 고정

실험을 재현 가능하도록 하기 위해 사용

torch.manual_seed(42)  # 시드 고정
print(torch.rand(3, 3))  # 동일한 난수 생성

📌 실행 결과

tensor([[0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.2566, 0.7936, 0.9408]])

➡ 다시 실행해도 동일한 결과를 얻을 수 있음!

=============================================================================
2-1-인플레이스(In-Place) 연산이란

인플레이스(In-Place) 연산은 기존의 **텐서를 변경(덮어쓰기)**하는 연산을 의미합니다.

즉, 새로운 텐서를 생성하지 않고, 기존 텐서의 메모리를 직접 수정하는 연산 방식입니다.

PyTorch에서는 인플레이스 연산의 경우 연산자 이름 끝에 _(언더스코어)를 추가합니다.

예를 들어, tensor.add_()와 같은 연산이 인플레이스 연산입니다.


import torch

# 기존 텐서
x = torch.tensor([1, 2, 3], dtype=torch.float32)

# 일반 연산 (새로운 텐서 반환)
y = x + 10
print("x (원본):", x)  # 원본 텐서 변경 없음
print("y (새로운 텐서):", y)  # 새로운 텐서 생성됨

# 인플레이스 연산
x.add_(10)
print("x (인플레이스 연산 후):", x)  # 기존 텐서가 변경됨

📌 실행 결과


x (원본): tensor([1., 2., 3.])
y (새로운 텐서): tensor([11., 12., 13.])
x (인플레이스 연산 후): tensor([11., 12., 13.])

✅ 일반 연산(x + 10)은 기존 텐서 x를 변경하지 않고 새로운 텐서 y를 반환
✅ 인플레이스 연산(x.add_(10))은 기존 텐서 x를 직접 변경

2. 주요 인플레이스 연산
연산	일반 연산	인플레이스 연산
덧셈	x = x + 5	x.add_(5)
뺄셈	x = x - 3	x.sub_(3)
곱셈	x = x * 2	x.mul_(2)
나눗셈	x = x / 4	x.div_(4)
지수 함수	x = x.exp()	x.exp_()
정규화	x = x.sqrt()	x.sqrt_()

3. 인플레이스 연산의 장점

✅ 메모리 사용량 감소

x = torch.randn(1000, 1000)

# 일반 연산 (새로운 메모리 할당)
y = x + 10  # 새로운 텐서가 생성됨

# 인플레이스 연산 (메모리 절약)
x.add_(10)  # 기존 텐서 x가 직접 변경됨

➡ 인플레이스 연산을 사용하면 새로운 텐서를 생성하지 않으므로 메모리 사용량이 줄어듦

4. 인플레이스 연산의 단점

❌ 자동 미분(Autograd) 사용 시 문제 발생 가능

x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)

y = x.add_(1)  # 인플레이스 연산
y.backward(torch.tensor([1.0, 1.0, 1.0]))  # 오류 발생 가능

➡ 인플레이스 연산은 PyTorch의 Autograd(자동 미분) 기능과 충돌할 가능성이 있음
➡ 그래디언트 계산 시 오류가 발생할 수 있으므로 학습 중에는 주의 필요!

5. 인플레이스 연산 사용 시 주의할 점
✅ 메모리 절약이 필요할 때 사용
✅ 학습 중(Backpropagation)에는 사용을 피하는 것이 좋음
✅ x.copy_(), x.zero_()와 같은 초기화 연산에서도 사용됨

6. 결론
인플레이스 연산(x.add_())은 기존 텐서를 변경하는 연산
메모리 절약이 가능하지만, 자동 미분과 충돌할 수 있어 학습 중에는 주의가 필요
모델 학습보다 추론(Inference) 시 메모리 절약을 위해 적절히 사용하면 유용


=============================================================================
2-2-PyTorch view() 함수


torch.view() 함수는 PyTorch에서
**텐서의 크기를 변경(Reshape)**할 때 사용하는 함수입니다.

하지만 view()는 기존 메모리를 유지하면서 크기만 변경하는 특징이 있습니다.

(즉, 데이터 자체는 변하지 않고, 텐서의 메모리 레이아웃을 재구성)

1. view()의 기본 문법

tensor.view(shape)
shape: 변경할 텐서의 크기 ((rows, columns, ...))

-1을 사용하면 자동으로 크기를 계산하여 설정

2. view() 예제: 1D → 2D 변환

import torch

x = torch.arange(6)  # 1D 텐서 (0~5)
print("Original Shape:", x.shape)

xreshaped = x.view(2, 3)  # 2행 3열로 변경

print("Reshaped Shape:", xreshaped.shape)
print(xreshaped)

📌 실행 결과


Original Shape: torch.Size([6])
Reshaped Shape: torch.Size([2, 3])
tensor([[0, 1, 2],
        [3, 4, 5]])

✅ 데이터는 유지되지만, Shape이 변경됨!


3. view(-1): 자동 크기 계산
-1을 사용하면 PyTorch가 자동으로 크기를 계산하여 설정합니다.

x = torch.arange(12)
xreshaped = x.view(3, -1)  # 열(-1)은 자동 계산됨

print(xreshaped.shape)
print(xreshaped)

📌 실행 결과

torch.Size([3, 4])
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])

✅ -1은 자동으로 4로 계산됨 (3 × 4 = 12 요소 개수 유지)

4. 3D → 2D 변환 예제


x = torch.arange(24).view(2, 3, 4)  # 3D 텐서 생성 (배치=2, 행=3, 열=4)

print("3D Tensor Shape:", x.shape)

xflattened = x.view(2, -1)  # 배치 유지, 나머지는 자동 조정
print("Flattened Shape:", xflattened.shape)

📌 실행 결과

3D Tensor Shape: torch.Size([2, 3, 4])
Flattened Shape: torch.Size([2, 12])

✅ 3D 텐서를 2D로 변환 (배치=2 유지, 3×4=12 개수 자동 계산)

5. view() 사용 시 주의할 점

1연속된 메모리(Contiguous) 필요

view()는 메모리 연속성이 유지되는 경우에만 사용 가능
만약 연속적이지 않다면 .contiguous().view()를 사용해야 함


x = torch.randn(2, 3, 4)  # (Batch=2, Rows=3, Cols=4)
xt = x.transpose(0, 1)    # (Rows=3, Batch=2, Cols=4) → 메모리 비연속

 차원 변경이 메모리 비연속성을 초래하는 이유
PyTorch에서 텐서는 기본적으로 연속적인 메모리(Contiguous Memory) 에 저장됨.
transpose(dim0, dim1)을 사용하면 실제 데이터 배열을 변경하지 않고,
인덱스만 변경하여 새로운 뷰를 생성.
따라서 메모리 상에서 데이터가 연속되지 않음(Non-contiguous).

xview = xt.contiguous().view(2, -1)  # 연속된 메모리로 변경 후 View 적용

2원소 개수(Elements) 유지 필요

view()를 사용하면 기존 텐서의 원소 개수가 그대로 유지되어야 함
개수가 맞지 않으면 오류 발생


x = torch.arange(10)
x.view(3, 3)  # 10개 원소를 (3×3)로 변경할 수 없으므로 오류 발생!

RuntimeError: shape '[3, 3]' is invalid for input of size 10

6. view()와 reshape() 차이

함수	특징
view()	메모리 연속성 유지 필요, 속도 빠름
reshape()	메모리 연속성 없어도 사용 가능

✅ 차이점 예제

x = torch.randn(2, 3)
xt = x.transpose(0, 1)  # 차원 변경 → 메모리 비연속

# view() 오류 발생
try:
    xview = xt.view(6)
except RuntimeError as e:
    print("view() Error:", e)

# reshape()는 자동으로 해결
xreshape = xt.reshape(6)
print("reshape() 성공:", xreshape.shape)

✅ 메모리 연속성이 없는 경우 view()는 실패하지만 reshape()는 가능!

✅ 결론
view()는 기존 메모리를 유지하면서 텐서의 shape을 변경하는 함수
메모리 연속성이 필요하며, contiguous()를 사용해야 할 수도 있음
원소 개수는 유지되어야 하며, -1을 사용하면 자동 계산 가능

비연속 메모리 텐서는 reshape()가 더 안전함

=============================================================================
2-3-squeeze() vs unsqueeze() 함수 차이점


PyTorch에서 squeeze()와 unsqueeze()는 텐서의 차원을 변경하는 함수입니다.

🔹 1️ squeeze(): 크기가 1인 차원 제거

📌 설명
squeeze()는 크기가 1인 차원을 자동으로 제거합니다.
데이터의 구조를 축소하는 데 사용됩니다.

📌 예제

import torch

x = torch.randn(1, 3, 1, 4)  # (Batch=1, Channels=3, Height=1, Width=4)

x_squeezed = x.squeeze()  # 크기가 1인 차원을 제거

print(x.shape)          # torch.Size([1, 3, 1, 4])
print(x_squeezed.shape) # torch.Size([3, 4])
(1, 3, 1, 4) → (3, 4) 로 변경됨 (크기가 1인 차원 제거)

📌 특정 차원만 제거하기

x_squeezed_1 = x.squeeze(0)  # 0번 차원(크기가 1인 경우만) 제거
x_squeezed_2 = x.squeeze(2)  # 2번 차원(크기가 1인 경우만) 제거

print(x_squeezed_1.shape)  # torch.Size([3, 1, 4])
print(x_squeezed_2.shape)  # torch.Size([1, 3, 4])
squeeze(dim)을 사용하면 특정 차원만 제거 가능.

🔹 2️ unsqueeze(): 크기가 1인 차원 추가

📌 설명
unsqueeze(dim)은 지정한 dim 위치에 크기가 1인 차원을 추가합니다.

배치 차원 추가, 채널 차원 추가 등에 사용됨.

📌 예제

y = torch.randn(3, 4)  # (3, 4) 형태
y_unsqueezed = y.unsqueeze(0)  # 0번 차원 추가

print(y.shape)           # torch.Size([3, 4])
print(y_unsqueezed.shape) # torch.Size([1, 3, 4])

(3, 4) → (1, 3, 4) 로 변경됨

📌 여러 차원 추가

y_unsqueezed_1 = y.unsqueeze(1)  # 1번 차원에 추가
y_unsqueezed_2 = y.unsqueeze(2)  # 2번 차원에 추가

print(y_unsqueezed_1.shape)  # torch.Size([3, 1, 4])
print(y_unsqueezed_2.shape)  # torch.Size([3, 4, 1])

unsqueeze(1): 채널 차원 추가할 때 유용
unsqueeze(0): 배치 차원 추가할 때 유용

✅ squeeze() vs unsqueeze() 차이 정리

함수	기능	결과 예시
squeeze()	크기 1인 차원을 제거	(1, 3, 1, 4) → (3, 4)
unsqueeze(dim)	특정 위치에 크기 1인 차원 추가	(3, 4) → (1, 3, 4)

📌 unsqueeze()를 활용한 배치 추가


x = torch.randn(3, 4)   # (3, 4)
x_batch = x.unsqueeze(0) # 배치 차원 추가 → (1, 3, 4)

📌 squeeze()를 활용한 차원 축소

x = torch.randn(1, 3, 1, 4)
x_squeezed = x.squeeze()  # 크기가 1인 차원 제거 → (3, 4)

=============================================================================
2-3-2-torch.cat([x.unsqueeze(0), y.unsqueeze(0)], dim=0) 예제



📌 예제 코드: unsqueeze(0)와 cat() 사용


import torch

# 2D 텐서 생성 (2x3 크기)
x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Shape: (2,3)
y = torch.tensor([[7, 8, 9], [10, 11, 12]])  # Shape: (2,3)

# unsqueeze(0)을 통해 새로운 차원 추가 후 cat() 적용
z = torch.cat([x.unsqueeze(0), y.unsqueeze(0)], dim=0)

# 결과 출력
print("Original x shape:", x.shape)
print("Original y shape:", y.shape)
print("After unsqueeze and cat, z shape:", z.shape)
print("z Tensor:\n", z)

📌 예상 출력 결과

plaintext

Original x shape: torch.Size([2, 3])
Original y shape: torch.Size([2, 3])
After unsqueeze and cat, z shape: torch.Size([2, 2, 3])
z Tensor:
 tensor([[[ 1,  2,  3],
          [ 4,  5,  6]],

         [[ 7,  8,  9],
          [10, 11, 12]]])
🔹 설명
unsqueeze(0):

x.unsqueeze(0) → (2,3) → (1,2,3)
y.unsqueeze(0) → (2,3) → (1,2,3)

cat(dim=0):

(1,2,3) + (1,2,3) → (2,2,3)

즉, 두 개의 (2,3) 텐서를 새로운 차원(0번 차원)으로 연결하여 (2,2,3)이 됨.

✅ 정리
연산	차원 변화
x.shape	(2,3)
x.unsqueeze(0).shape	(1,2,3)
y.unsqueeze(0).shape	(1,2,3)
torch.cat([x.unsqueeze(0), y.unsqueeze(0)], dim=0)	(2,2,3)

✅ 이 방법은 배치(batch) 차원을 추가할 때 유용하며,
CNN 입력 데이터처럼 사용될 수도 있습니다.

=============================================================================
2-4-2-텐서(Tensor) 크기 및 요소 변화 개념 설명



**텐서(Tensor)**는 다차원 배열을 의미하며,
**벡터(Vector), 행렬(Matrix), 다차원 배열(ND-array)**을 일반화한 개념입니다.

PyTorch에서는 torch.Tensor를 사용하여 다차원 데이터를 처리합니다.

1. 텐서의 기본 개념

차원	의미	예제

0D (스칼라, Scalar)	하나의 숫자	torch.tensor(5)
1D (벡터, Vector)	숫자의 리스트	torch.tensor([1, 2, 3])
2D (행렬, Matrix)	행과 열이 있는 배열	torch.tensor([[1, 2], [3, 4]])
3D 이상 (다차원 텐서, ND-Tensor)	여러 개의 행렬로 구성된 구조
torch.tensor([[[1,2], [3,4]], [[5,6], [7,8]]])

2. 텐서 크기(Shape) 이해
PyTorch에서 tensor.shape을 확인하면 텐서의 차원과 크기를 알 수 있습니다.

import torch

# 0D 텐서 (스칼라)
scalar = torch.tensor(42)
print("Scalar Shape:", scalar.shape)  # ()

# 1D 텐서 (벡터)
vector = torch.tensor([1, 2, 3])
print("Vector Shape:", vector.shape)  # (3,)

# 2D 텐서 (행렬)
matrix = torch.tensor([[1, 2, 3], [4, 5, 6]])
print("Matrix Shape:", matrix.shape)  # (2, 3)

# 3D 텐서 (다차원 배열)
tensor_3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])
print("3D Tensor Shape:", tensor_3d.shape)  # (2, 2, 2)

📌 실행 결과

Scalar Shape: torch.Size([])
Vector Shape: torch.Size([3])
Matrix Shape: torch.Size([2, 3])
3D Tensor Shape: torch.Size([2, 2, 2])

✅ 각 텐서의 shape을 보면 차원이 어떻게 구성되는지 확인 가능!

3. 텐서 요소 변화 예시

✅ 1) reshape()을 활용한 형태 변경

x = torch.arange(6)  # [0, 1, 2, 3, 4, 5]
print("Original Shape:", x.shape)  # (6,)

x_reshaped = x.reshape(2, 3)  # 2행 3열로 변경
print("Reshaped Shape:", x_reshaped.shape)  # (2, 3)
print(x_reshaped)

📌 실행 결과

Original Shape: torch.Size([6])
Reshaped Shape: torch.Size([2, 3])
tensor([[0, 1, 2],
        [3, 4, 5]])

✅ reshape을 사용하면 텐서의 shape을 변경 가능
✅ 데이터의 개수(요소 개수)는 그대로 유지되지만, 차원만 변경됨


✅ 2) unsqueeze()를 활용한 차원 추가


x = torch.tensor([1, 2, 3])
print("Original Shape:", x.shape)  # (3,)

x_unsqueezed = x.unsqueeze(0)  # 차원 추가 (배치 차원 추가)
print("Unsqueezed Shape:", x_unsqueezed.shape)  # (1, 3)

📌 실행 결과

Original Shape: torch.Size([3])
Unsqueezed Shape: torch.Size([1, 3])

✅ unsqueeze(dim=0)을 사용하면 (3,) → (1, 3)로 차원이 추가됨
✅ CNN 모델에서 배치 차원을 추가할 때 자주 사용됨

✅ 3) squeeze()를 활용한 차원 축소


x = torch.tensor([[1, 2, 3]])  # (1, 3)
print("Original Shape:", x.shape)

x_squeezed = x.squeeze()
print("Squeezed Shape:", x_squeezed.shape)  # (3,)

📌 실행 결과

Original Shape: torch.Size([1, 3])
Squeezed Shape: torch.Size([3])
✅ squeeze()를 사용하면 불필요한 차원이 제거됨
✅ (1, 3) → (3)로 변경됨

✅ 4) permute()를 활용한 차원 순서 변경

x = torch.randn(3, 4, 5)  # (채널, 높이, 너비)
print("Original Shape:", x.shape)  # (3, 4, 5)

x_permuted = x.permute(1, 2, 0)  # (높이, 너비, 채널)로 변경

print("Permuted Shape:", x_permuted.shape)  # (4, 5, 3)

📌 실행 결과

Original Shape: torch.Size([3, 4, 5])
Permuted Shape: torch.Size([4, 5, 3])
✅ CNN에서 이미지 데이터를 변환할 때 자주 사용됨
✅ 채널-높이-너비 구조를 높이-너비-채널 구조로 변경 가능

4. 텐서 연산을 통한 요소 변화

x = torch.tensor([[1, 2], [3, 4]])
print("Original Tensor:")
print(x)

# 텐서 값 변경 (연산 수행)
y = x * 2  # 요소별 연산 (모든 원소를 2배)
print("After Multiplication by 2:")
print(y)

# 요소별 제곱 연산
z = x ** 2
print("After Squaring:")
print(z)

📌 실행 결과


Original Tensor:
tensor([[1, 2],
        [3, 4]])

After Multiplication by 2:
tensor([[2, 4],
        [6, 8]])

After Squaring:
tensor([[ 1,  4],
        [ 9, 16]])

✅ 텐서의 요소 값을 연산을 통해 변경 가능
✅ 브로드캐스팅을 통해 모든 원소에 동일한 연산 적용 가능

5. 결론
📌 텐서 크기 개념 정리
0D 텐서 (스칼라): 숫자 하나
1D 텐서 (벡터): 리스트 형태의 배열
2D 텐서 (행렬): 행과 열이 있는 배열
3D 이상 텐서: 다차원 배열

📌 주요 텐서 변환 연산
연산	설명	예제
reshape()	크기 변경 (요소 개수 유지)	(6,) → (2, 3)
unsqueeze()	차원 추가	(3,) → (1, 3)
squeeze()	차원 제거	(1, 3, 1) → (3,)
permute()	차원 순서 변경	(3, 4, 5) → (4, 5, 3)

📌 텐서 연산
덧셈, 뺄셈, 곱셈 등 요소별 연산 가능
브로드캐스팅을 통해 자동으로 크기 맞춤
CNN, RNN에서 데이터 변환 시 필수적


=============================================================================
2-4-3-입력 데이터 shape 및 출력 데이터 shape 분석

위 코드에서 입력 데이터 X와 출력 데이터 Y의 shape(모양)을 분석하고,

모델의 각 레이어를 통과할 때 데이터가 어떻게 변하는지 설명하겠습니다.

1. 입력 데이터 X와 정답 데이터 Y의 shape



X = torch.tensor([[0, 0],
                  [0, 1],
                  [1, 0],
                  [1, 1]], dtype=torch.float32)  # 입력값
Y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)  # 정답값

데이터	내용	Shape
X	4개의 샘플, 각 샘플은 2개의 입력 특성(0 또는 1)	(4, 2)
Y	4개의 샘플, 각 샘플의 정답(0 또는 1)	(4, 1)

📌 즉, X는 (batch_size, input_dim) = (4, 2) 형태의 입력을 가짐

📌 정답 Y는 (batch_size, output_dim) = (4, 1) 형태의 출력을 기대함


2. forward() 함수에서 데이터 흐름과 shape 변화

def forward(self, x):
    x = self.activation(self.hidden(x))  # 은닉층 활성화
    x = self.activation(self.output(x))  # 출력층 활성화
    return x

(1) 입력층 → 은닉층 변환 (self.hidden(x))


self.hidden = nn.Linear(2, 2)  # (input_dim=2 → hidden_dim=2)

X.shape = (4, 2) → hidden(x)을 통과하면 (4, 2)
선형 변환 공식:

Z1 = X * W1 + b1



(2) 은닉층 활성화 (self.activation(hidden(x)))
활성화 함수 Sigmoid 적용 → shape 변화 없음
x.shape = (4, 2)

(3) 은닉층 → 출력층 변환 (self.output(x))

 self.output = nn.Linear(2, 1)  # (hidden_dim=2 → output_dim=1)
x.shape = (4, 2) → output(x)을 통과하면 (4, 1)

선형 변환 공식:
Z2 = A1 * W2 + b2

(4,2) × (2,1) = (4,1)

​
 의 shape: (4, 1)

(4) 출력층 활성화 (self.activation(output(x)))
활성화 함수 Sigmoid 적용 → shape 변화 없음
최종 출력 x.shape = (4, 1)

3. 데이터 흐름 Shape 변화 요약
레이어	연산	Shape 변화

입력 데이터	X	(4, 2)
은닉층 선형 변환	self.hidden(x)	(4, 2)
은닉층 활성화 함수	self.activation(hidden(x))	(4, 2)
출력층 선형 변환	self.output(x)	(4, 1)
출력층 활성화 함수	self.activation(output(x))	(4, 1)
최종 출력	Y_hat	(4, 1)

4. 실제 shape 확인 코드
모델을 실행하면서 각 레이어에서 데이터 shape 변화를 직접 확인해 보겠습니다.


import torch
import torch.nn as nn

class XOR_NN(nn.Module):
    def __init__(self):
        super(XOR_NN, self).__init__()
        self.hidden = nn.Linear(2, 2)  # 입력 2 → 은닉층 2
        self.output = nn.Linear(2, 1)  # 은닉층 2 → 출력층 1
        self.activation = nn.Sigmoid()  # 활성화 함수

    def forward(self, x):
        print("입력 데이터 shape:", x.shape)  # (4, 2)
        x = self.hidden(x)
        print("은닉층 통과 후 shape:", x.shape)  # (4, 2)
        x = self.activation(x)
        print("은닉층 활성화 후 shape:", x.shape)  # (4, 2)
        x = self.output(x)
        print("출력층 통과 후 shape:", x.shape)  # (4, 1)
        x = self.activation(x)
        print("출력층 활성화 후 shape:", x.shape)  # (4, 1)
        return x

# 모델 생성
model = XOR_NN()

# 입력 데이터
X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)

# 모델 실행
output = model(X)

📌 실행 결과


입력 데이터 shape: torch.Size([4, 2])
은닉층 통과 후 shape: torch.Size([4, 2])
은닉층 활성화 후 shape: torch.Size([4, 2])
출력층 통과 후 shape: torch.Size([4, 1])
출력층 활성화 후 shape: torch.Size([4, 1])

5. 결론
✅ 입력 데이터 X는 (4, 2)로 시작하며, 최종 출력 Y_hat은 (4, 1)로 변환됨
✅ 은닉층에서 뉴런 개수(2)를 유지하며, 출력층에서 1개의 값으로 줄어듦
✅ 각 레이어를 통과할 때 shape 변화는 다음과 같이 진행됨

(4, 2) → (4, 2) → (4, 2) → (4, 1) → (4, 1)

✅ 모델 실행 시 print()로 shape 변화를 직접 확인 가능

=============================================================================
2-4-4-3차원 텐서(3D Tensor)에서 인덱스 별 위치 이해

3차원 텐서(3D Tensor)에서 인덱스 별 위치 이해

✅ 3차원 텐서의 기본 개념
3차원 텐서는 다음과 같은 **3개의 차원(Dimension)**을 가집니다.

3D Tensor Shape: (D1, D2, D3)

첫 번째 차원 (D_1) → 일반적으로 배치 크기(Batch Size), 채널 수(Channel)
두 번째 차원 (D_2) → 행(Row) 또는 높이(Height)
세 번째 차원 (D_3) → 열(Column) 또는 너비(Width)

즉, (배치, 높이, 너비) = (D_1, D_2, D_3) 형태로 이해할 수 있습니다.

1. 3D 텐서 생성 및 각 인덱스 별 위치 확인

import torch

# 3D 텐서 생성 (배치=2, 높이=3, 너비=4)
tensor_3d = torch.tensor([
    [[1, 2, 3, 4],
     [5, 6, 7, 8],
     [9, 10, 11, 12]],  # 첫 번째 배치

    [[13, 14, 15, 16],
     [17, 18, 19, 20],
     [21, 22, 23, 24]]  # 두 번째 배치
])

print("3D Tensor Shape:", tensor_3d.shape)  # (2, 3, 4)

📌 생성된 텐서 구조



배치 0:
[[ 1   2   3   4 ]
 [ 5   6   7   8 ]
 [ 9  10  11  12 ]]

배치 1:
[[13  14  15  16 ]
 [17  18  19  20 ]
 [21  22  23  24 ]]

➡ Shape: (2, 3, 4)

➡ (배치=2, 높이=3, 너비=4)


2. 각 차원별 인덱스 위치

텐서에서 특정 인덱스의 값을 참조할 때 (배치, 높이, 너비) 순서로 접근합니다.

인덱스	위치	값
tensor_3d[0, 0, 0]	첫 번째 배치, 첫 번째 행, 첫 번째 열	1
tensor_3d[0, 0, 1]	첫 번째 배치, 첫 번째 행, 두 번째 열	2
tensor_3d[0, 1, 2]	첫 번째 배치, 두 번째 행, 세 번째 열	7
tensor_3d[1, 2, 3]	두 번째 배치, 세 번째 행, 네 번째 열	24



print("tensor_3d[0, 0, 0]:", tensor_3d[0, 0, 0])  # 1
print("tensor_3d[0, 0, 1]:", tensor_3d[0, 0, 1])  # 2
print("tensor_3d[0, 1, 2]:", tensor_3d[0, 1, 2])  # 7
print("tensor_3d[1, 2, 3]:", tensor_3d[1, 2, 3])  # 24

📌 실행 결과

tensor_3d[0, 0, 0]: tensor(1)
tensor_3d[0, 0, 1]: tensor(2)
tensor_3d[0, 1, 2]: tensor(7)
tensor_3d[1, 2, 3]: tensor(24)

3. 특정 차원 선택하여 출력

✅ (1) 특정 배치 선택 (tensor_3d[0])

print(tensor_3d[0])  # 첫 번째 배치 데이터 출력

📌 실행 결과


tensor([[ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12]])

➡ 첫 번째 배치(0번째 차원)가 출력됨

✅ (2) 특정 행 선택 (tensor_3d[1, 1])

print(tensor_3d[1, 1])  # 두 번째 배치의 두 번째 행

📌 실행 결과

tensor([17, 18, 19, 20])

➡ 두 번째 배치에서 두 번째 행(1번째 행) 선택됨

✅ (3) 특정 열 선택 (tensor_3d[:, :, 2])

print(tensor_3d[:, :, 2])  # 모든 배치에서 3번째 열 선택

📌 실행 결과


tensor([[ 3,  7, 11],
        [15, 19, 23]])

➡ 모든 배치에서 3번째 열(인덱스=2) 값만 선택됨

4. permute()를 활용한 위치 변경
✅ permute(1, 2, 0): (배치, 높이, 너비) → (높이, 너비, 배치)


tensor_permuted = tensor_3d.permute(1, 2, 0)  # (높이, 너비, 배치)
print("Permuted Shape:", tensor_permuted.shape)

📌 실행 결과

Permuted Shape: torch.Size([3, 4, 2])

➡ 배치가 마지막으로 이동하고, 높이가 첫 번째 차원으로 변경됨

✅ permute(2, 0, 1): (배치, 높이, 너비) → (너비, 배치, 높이)

tensor_permuted = tensor_3d.permute(2, 0, 1)  # (너비, 배치, 높이)
print("Permuted Shape:", tensor_permuted.shape)

📌 실행 결과

Permuted Shape: torch.Size([4, 2, 3])

➡ 너비가 첫 번째 차원으로 이동하고, 배치와 높이의 순서도 바뀜

5. 결론
✅ 3D 텐서는 (배치, 높이, 너비) = (D_1, D_2, D_3) 형태
✅ 인덱싱(tensor_3d[i, j, k])을 사용하여 특정 위치 선택 가능
✅ permute()를 사용하면 차원의 순서를 변경 가능
✅ 특정 차원 선택 시 :를 활용하여 전체 배치 또는 특정 행/열만 가져올 수 있음


=============================================================================
2-5-split() vs chunk() 차이점

📌 split() vs chunk() 차이점

torch.split()과 torch.chunk()는

PyTorch에서 텐서를 특정 크기 또는 개수로 나누는 함수입니다.

🔹 1️ split() 함수

split(batch_size, dim) : 주어진 크기(batch_size)로 텐서를 나눔.

마지막 부분의 크기는 남은 요소 개수에 따라 조정될 수 있음.

📌 예제

import torch

x = torch.FloatTensor(10, 4)  # (10, 4) 크기의 텐서 생성

splits = x.split(4, dim=0)  # 첫 번째 차원(행 기준)으로 크기 4씩 나눔

for s in splits:
    print(s.size())

📌 결과

torch.Size([4, 4])
torch.Size([4, 4])
torch.Size([2, 4])  # 마지막 남은 부분 (2개)

첫 번째와 두 번째 텐서는 (4, 4), 마지막 텐서는 (2, 4).

마지막 조각은 크기가 다를 수 있음. (총 10개 데이터를 4개씩 나누면 4+4+2)


🔹 2 ️chunk() 함수
chunk(n_chunks, dim): 지정한 개수(n_chunks)로 텐서를 균등하게 나눔.

split()과 다르게 크기가 자동으로 균등하게 배분됨.

📌 예제

x = torch.FloatTensor(8, 4)  # (8, 4) 크기의 텐서 생성
chunks = x.chunk(3, dim=0)  # 3개로 나눔

for c in chunks:
    print(c.size())

📌 결과

torch.Size([3, 4])
torch.Size([3, 4])
torch.Size([2, 4])  # 남은 부분

split()과 유사하지만, 균등하게 분할할 수 있도록 자동으로 크기를 맞춤.

첫 번째, 두 번째는 (3, 4), 마지막은 (2, 4).

→ 8을 3으로 나누면 3+3+2로 나눠짐.

✅ split() vs chunk() 비교

함수	동작 방식	마지막 조각 크기

split(batch_size, dim)	지정된 크기 batch_size만큼 나누고

남은 부분을 마지막에 배정	마지막 부분만 다를 수 있음

chunk(n_chunks, dim)	지정된 개수 n_chunks로 균등하게 분할
균등하게 나누되, 마지막 조각이 다를 수 있음

✅ 결론
정확한 크기(batch_size)로 나누려면 split() 사용.
균등하게 n개로 나누려면 chunk() 사용.

=============================================================================
2-6-torch.index_select() 함수


PyTorch의 index_select() 함수는 특정 차원의 특정 인덱스만 선택하여

새로운 텐서를 생성하는 함수입니다.

🔹 1️ index_select() 기본 문법

torch.index_select(input, dim, index)

input : 원본 텐서
dim : 선택할 차원 (0=행, 1=열, ...)
index : 선택할 인덱스 리스트
(torch.LongTensor 또는 torch.tensor([...], dtype=torch.long) 사용)


🔹 2️ 행 선택 예제 (dim=0)

import torch

x = torch.arange(12).reshape(4, 3)  # (4,3) 텐서 생성
print("Original Tensor:\n", x)

# 선택할 행 인덱스 정의 (1번째, 3번째 행 선택)
index = torch.tensor([1, 3], dtype=torch.long)

# 행 선택 (dim=0)
selected_rows = torch.index_select(x, dim=0, index=index)

print("Selected Rows:\n", selected_rows)


📌 출력 결과

Original Tensor:
 tensor([[ 0,  1,  2],
         [ 3,  4,  5],
         [ 6,  7,  8],
         [ 9, 10, 11]])

Selected Rows:
 tensor([[ 3,  4,  5],
         [ 9, 10, 11]])

index=[1,3]이므로 1번째, 3번째 행만 선택됨.


🔹 3️ 열 선택 예제 (dim=1)

index = torch.tensor([0, 2], dtype=torch.long)  # 첫 번째, 세 번째 열 선택

selected_cols = torch.index_select(x, dim=1, index=index)

print("Selected Columns:\n", selected_cols)

📌 출력 결과

Selected Columns:
 tensor([[ 0,  2],
         [ 3,  5],
         [ 6,  8],
         [ 9, 11]])

dim=1 → 열 방향으로 선택

index=[0,2]이므로 첫 번째, 세 번째 열만 선택됨.

🔹 4️ index_select() vs tensor[indices] 차이


# 직접 인덱싱
selected_rows = x[[1, 3]]
print(selected_rows)

x[[1, 3]]와 torch.index_select(x, 0, index)의 차이점

index_select()는 메모리 연속성이 유지됨.

x[[...]]는 메모리 비연속적인 텐서를 생성할 수 있음.

✅ 정리

함수	기능	예제
index_select(x, dim=0, index)
특정 행 선택	index_select(x, 0, torch.tensor([1,3]))

index_select(x, dim=1, index)
특정 열 선택	index_select(x, 1, torch.tensor([0,2]))

x[[indices]]	특정 행 선택 (직접 인덱싱)	x[[1,3]]

✅ index_select()는 특정 차원의 원하는 요소만 선택할 때 사용하면 효율적!

=============================================================================
2-7-torch.cat() (Concatenate) 함수

📌 torch.cat() (Concatenate) 함수

PyTorch의 torch.cat() 함수는 여러 개의 텐서를
특정 차원(dim)으로 연결(Concatenate)하는 함수입니다.


🔹 1️ torch.cat() 기본 문법

torch.cat(tensors, dim)
tensors : 연결할 텐서들의 리스트 [tensor1, tensor2, ...]
dim : 연결할 차원 (0=행 방향, 1=열 방향, ...)


🔹 2️ 행 방향(dim=0)으로 연결

import torch

a = torch.tensor([[1, 2], [3, 4]])  # (2,2)
b = torch.tensor([[5, 6], [7, 8]])  # (2,2)

# 행 방향(dim=0)으로 연결
c = torch.cat([a, b], dim=0)

print("Concatenated Tensor (dim=0):\n", c)
print("Shape:", c.shape)

📌 출력 결과

Concatenated Tensor (dim=0):
 tensor([[1, 2],
         [3, 4],
         [5, 6],
         [7, 8]])

Shape: torch.Size([4, 2])

(2,2) + (2,2) → (4,2)

행이 늘어남(수직 방향으로 연결).


🔹 3️ 열 방향(dim=1)으로 연결

c = torch.cat([a, b], dim=1)  # 열 방향으로 연결

print("Concatenated Tensor (dim=1):\n", c)
print("Shape:", c.shape)


📌 출력 결과

Concatenated Tensor (dim=1):
 tensor([[1, 2, 5, 6],
         [3, 4, 7, 8]])

Shape: torch.Size([2, 4])
(2,2) + (2,2) → (2,4)

열이 늘어남(수평 방향으로 연결).

🔹 4️ cat()을 사용할 때 주의할 점

✔️ 차원이 동일해야 함

연결할 텐서는 모든 차원이 동일해야 하며, dim 방향만 다를 수 있음.

예제에서 (2,2) 크기의 a, b를 dim=0으로 연결하면 행 개수가 증가하고 (4,2)이 됨.

오류 예제

a = torch.tensor([[1, 2]])  # (1,2)
b = torch.tensor([[3], [4]])  # (2,1)

c = torch.cat([a, b], dim=0)  # 서로 다른 shape → 오류 발생!

🔴 오류 발생

RuntimeError: Sizes of tensors must match except in dimension 0
해결 방법: reshape() 또는 unsqueeze()를 사용하여 크기를 맞춰야 함.

✅ 정리
함수	기능	결과
torch.cat([a, b], dim=0)	행(수직)으로 연결	(2,2) + (2,2) → (4,2)
torch.cat([a, b], dim=1)	열(수평)으로 연결	(2,2) + (2,2) → (2,4)

✅ torch.cat()은 데이터 합치기, 배치 데이터 병합 등에 유용!

=============================================================================
2-8-torch.stack() 함수


PyTorch의 torch.stack() 함수는

새로운 차원을 추가하면서 여러 텐서를 쌓는(stack) 함수입니다.

🔹 1️ torch.stack() 기본 문법

torch.stack(tensors, dim)
tensors : 스택할 텐서들의 리스트 [tensor1, tensor2, ...]
dim : 새로운 차원을 추가할 위치

✅ cat()과의 차이점:

cat()은 기존 차원에서 연결 (새로운 차원 추가 ❌).

stack()은 새로운 차원 추가하면서 연결 (새로운 차원 추가 ⭕).


🔹 2️ stack() 예제

📌 예제 1: 기본 사용 (dim=0)

import torch

a = torch.tensor([1, 2, 3])  # (3,)
b = torch.tensor([4, 5, 6])  # (3,)

c = torch.stack([a, b], dim=0)  # 새로운 차원 추가

print("Stacked Tensor (dim=0):\n", c)
print("Shape:", c.shape)

📌 출력 결과

Stacked Tensor (dim=0):
 tensor([[1, 2, 3],
         [4, 5, 6]])

Shape: torch.Size([2, 3])

새로운 차원이 0번(dim=0) 위치에 추가됨.

결과 shape: (2,3)
→ 기존 (3,) 텐서 2개를 쌓아서 (2,3) 텐서 생성.


📌 예제 2: dim=1일 때

c = torch.stack([a, b], dim=1)  # 새로운 차원을 1번 위치에 추가

print("Stacked Tensor (dim=1):\n", c)
print("Shape:", c.shape)

📌 출력 결과


Stacked Tensor (dim=1):
 tensor([[1, 4],
         [2, 5],
         [3, 6]])

Shape: torch.Size([3, 2])

새로운 차원이 1번(dim=1) 위치에 추가됨.
결과 shape: (3,2) → 기존 (3,) 텐서 2개를 (3,2) 형태로 변환.

📌 예제 3: 2D 텐서에 적용

a = torch.tensor([[1, 2], [3, 4]])  # (2,2)
b = torch.tensor([[5, 6], [7, 8]])  # (2,2)

c = torch.stack([a, b], dim=0)
print("Stacked Tensor (dim=0):\n", c)
print("Shape:", c.shape)

📌 출력 결과

Stacked Tensor (dim=0):
 tensor([[[1, 2],
          [3, 4]],

         [[5, 6],
          [7, 8]]])
Shape: torch.Size([2, 2, 2])

dim=0으로 stack하면 (2,2) → (2,2,2) (새로운 차원 추가됨)

✅ 정리
함수	기능	결과 예시
torch.stack([a, b], dim=0)	새로운 차원을 추가하며 쌓음	(3,) → (2,3)
torch.stack([a, b], dim=1)	열 방향으로 차원 추가	(3,) → (3,2)
torch.cat([a, b], dim=0)	기존 차원에서 연결	(3,) + (3,) → (6,)

🚀 stack()은 새로운 차원이 필요할 때 사용! (cat()과 차이점 주의) 🎯


📌 2D 텐서에 stack(dim=1) 적용 예제


import torch

# 2D 텐서 생성 (2x2 크기)
a = torch.tensor([[1, 2], [3, 4]])  # Shape: (2,2)
b = torch.tensor([[5, 6], [7, 8]])  # Shape: (2,2)

# dim=1 방향으로 stack 적용
c = torch.stack([a, b], dim=1)

# 결과 출력
print("Stacked Tensor (dim=1):\n", c)
print("Shape:", c.shape)


a = torch.tensor([[1, 2], [3, 4]])  # Shape: (2,2)

(2,1,2)
[
 [      # 0 배치
 [1,2]
 ],

 [      # 1 배치
 [3,4]
 ],

]

추가, dim=1 , 결국 행방향

[
 [      # 0 배치
 [1,2],
 [5,6]
 ],

 [      # 1 배치
 [3,4],
 [7,8]
 ],

]

📌 예상 출력 결과


Stacked Tensor (dim=1):
 tensor([[[1, 2],
          [5, 6]],

         [[3, 4],
          [7, 8]]])

Shape: torch.Size([2, 2, 2])

dim=1에 새로운 차원을 추가하면서 쌓였기 때문에 (2,2,2) 크기의 텐서가 생성됨.

(2,2) → (2,2,2) 형태로 변환.

=============================================================================
2-9-expand() 예제


import torch

# 3D 텐서 생성 (2, 1, 2)
x = torch.FloatTensor([[[1, 2]], [[3, 4]]])
print("Original x shape:", x.shape)

# expand()를 사용하여 새로운 차원 확장 (2, 3, 2)
y = x.expand(2, 3, 2)

# 결과 출력
print("Expanded y shape:", y.shape)

print("Expanded y Tensor:\n", y)

📌 예상 출력 결과

Original x shape: torch.Size([2, 1, 2])
Expanded y shape: torch.Size([2, 3, 2])
Expanded y Tensor:
 tensor([[[1., 2.],
          [1., 2.],
          [1., 2.]],

         [[3., 4.],
          [3., 4.],
          [3., 4.]]])
🔹 설명
원본 텐서 x의 크기: (2, 1, 2)

2: 배치 크기
1: 확장될 차원 (단일 차원)
2: 컬럼 개수
expand(2, 3, 2) 적용 후

dim=1에서 1 → 3으로 확장됨.
메모리 재할당 없이 기존 값을 반복하여 확장 (Broadcasting).

✅ 주의할 점
expand()는 메모리   없이 기존 텐서를 확장하며, 실제로는 뷰(View) 를 제공.
변경 불가능(Immutable) → expand()한 텐서에서 값을 직접 수정할 수 없음.
메모리 연속성 필요할 경우 clone() 사용:


y_clone = y.clone()

✅ expand()는 브로드캐스팅(Broadcasting)을 활용한 메모리 효율적인 연산에 유용

=============================================================================
2-10-torch.randperm() 예제


import torch

# 0부터 9까지의 숫자를 무작위로 섞기 (순열 생성)
perm = torch.randperm(10)

# 결과 출력
print("Random Permutation:", perm)

📌 예상 출력 결과

Random Permutation: tensor([2, 5, 8, 1, 3, 7, 4, 6, 0, 9])

torch.randperm(n)은 0부터 n-1까지의 숫자를 랜덤한 순서로 정렬한 텐서를 반환합니다.

**순열(Permutation)**을 생성하는 데 사용됩니다.

🔹 응용 예제
✅ 데이터셋의 인덱스 랜덤 섞기

data = torch.tensor([10, 20, 30, 40, 50])
indices = torch.randperm(len(data))

shuffled_data = data[indices]

print("Original Data:", data)
print("Shuffled Data:", shuffled_data)

📌 예상 출력 결과

Original Data: tensor([10, 20, 30, 40, 50])
Shuffled Data: tensor([30, 10, 50, 20, 40])  # 무작위 순서
torch.randperm()을 활용하여 데이터를 랜덤하게 섞을 때 유용합니다.

✅ 정리

함수	기능
torch.randperm(n)	0부터 n-1까지의 랜덤 순열 생성
data[torch.randperm(len(data))]	데이터 무작위 섞기

✅ torch.randperm()은 데이터 순서를 무작위로 바꿀 때 유용한 함수!

=============================================================================
2-11-torch.argmax(dim=n) 예제



import torch

# 3x3 텐서 생성
x = torch.tensor([[1, 7, 3],
                  [4, 2, 9],
                  [8, 6, 5]])

# argmax(dim=0): 열 방향(세로)에서 최대값의 인덱스 찾기

argmax_dim0 = torch.argmax(x, dim=0)

# argmax(dim=1): 행 방향(가로)에서 최대값의 인덱스 찾기

argmax_dim1 = torch.argmax(x, dim=1)

# 결과 출력
print("Tensor:\n", x)
print("\nArgmax along dim=0 (column-wise):", argmax_dim0)
print("Argmax along dim=1 (row-wise):", argmax_dim1)

📌 예상 출력 결과

Tensor:
 tensor([[1, 7, 3],
         [4, 2, 9],
         [8, 6, 5]])

Argmax along dim=0 (column-wise): tensor([2, 0, 1])
Argmax along dim=1 (row-wise): tensor([1, 2, 0])

🔹 설명
torch.argmax(x, dim=0) → 열 방향 (dim=0)에서 최대값의 인덱스 반환

첫 번째 열 [1, 4, 8] → 최대값: 8 (인덱스 2)
두 번째 열 [7, 2, 6] → 최대값: 7 (인덱스 0)
세 번째 열 [3, 9, 5] → 최대값: 9 (인덱스 1)

결과: tensor([2, 0, 1])

torch.argmax(x, dim=1) → 행 방향 (dim=1)에서 최대값의 인덱스 반환

첫 번째 행 [1, 7, 3] → 최대값: 7 (인덱스 1)
두 번째 행 [4, 2, 9] → 최대값: 9 (인덱스 2)
세 번째 행 [8, 6, 5] → 최대값: 8 (인덱스 0)

결과: tensor([1, 2, 0])

✅ 정리
연산	설명	결과 예시

torch.argmax(x, dim=0)	열 방향(세로)에서 최대값의 인덱스 반환
tensor([2, 0, 1])
torch.argmax(x, dim=1)	행 방향(가로)에서 최대값의 인덱스 반환
tensor([1, 2, 0])

✅ torch.argmax(dim=n)을 사용하면 특정 차원에서
최대값의 인덱스를 쉽게 찾을 수 있음!

=============================================================================
2-12-torch.topk() 예제


import torch

# 3x3 텐서 생성
x = torch.tensor([[1, 7, 3],
                  [4, 2, 9],
                  [8, 6, 5]])

# 각 행(row)에서 최대값 1개(`k=1`) 찾기 (dim=-1 → 마지막 차원 기준)

topk_values, topk_indices = torch.topk(x, k=1, dim=-1)

# 결과 출력
print("Tensor:\n", x)
print("\nTop-1 values along dim=-1 (row-wise):", topk_values)
print("Top-1 indices along dim=-1:", topk_indices)

📌 예상 출력 결과


Tensor:
 tensor([[1, 7, 3],
         [4, 2, 9],
         [8, 6, 5]])

Top-1 values along dim=-1 (row-wise): tensor([[7],
                                             [9],
                                             [8]])

Top-1 indices along dim=-1: tensor([[1],
                                    [2],
                                    [0]])
🔹 설명
torch.topk(x, k=1, dim=-1)
dim=-1은 마지막 차원(여기서는 행 방향)을 기준으로 동작.

k=1이므로 각 행(row)에서 가장 큰 값 1개를 선택.

topk_values: 각 행에서 최대값 반환.
topk_indices: 각 행에서 최대값의 인덱스 반환.

📌 결과 해석
첫 번째 행 [1, 7, 3] → 최대값: 7 (인덱스 1)
두 번째 행 [4, 2, 9] → 최대값: 9 (인덱스 2)
세 번째 행 [8, 6, 5] → 최대값: 8 (인덱스 0)

✅ 정리
연산	설명	결과 예시
torch.topk(x, k=1, dim=0)
열 방향에서 최댓값 1개 선택	tensor([[8, 7, 9]])

torch.topk(x, k=1, dim=1)
행 방향에서 최댓값 1개 선택	tensor([[7], [9], [8]])

✅ torch.topk()를 사용하면 k개의 최댓값과 해당 인덱스를 쉽게 찾을 수 있음!


📌 torch.topk(x, k=x.size(target_dim), largest=True) 옵션 설명
torch.topk(x, k, dim, largest=True/False)

k=x.size(target_dim) → 선택된 차원의 모든 원소를 반환 (정렬 효과).

k = x.size(1)의 값은 3입니다.

🔹 해석
x.size(1)은 x 텐서의 두 번째 차원(열 개수)을 반환합니다.
x는 (3,3) 크기의 텐서이므로 x.size(1) = 3입니다.
따라서 torch.topk()에서 k=3을 설정하면,
각 행에서 3개의 최댓값을 내림차순으로 정렬합니다. ​

largest=True → 내림차순(큰 값부터 정렬).
largest=False → 오름차순(작은 값부터 정렬).

🔹 1️ 예제 코드



import torch

# 3x3 텐서 생성
x = torch.tensor([[1, 7, 3],
                  [4, 2, 9],
                  [8, 6, 5]])

# 행(dim=1) 기준으로 정렬 (내림차순)
sorted_values, sorted_indices = torch.topk(x, k=x.size(1), dim=1, largest=True)

# 결과 출력
print("Tensor:\n", x)
print("\nSorted values (descending order):\n", sorted_values)
print("Sorted indices:\n", sorted_indices)

📌 예상 출력 결과

Tensor:
 tensor([[1, 7, 3],
         [4, 2, 9],
         [8, 6, 5]])

Sorted values (descending order):
 tensor([[7, 3, 1],
         [9, 4, 2],
         [8, 6, 5]])

Sorted indices:
 tensor([[1, 2, 0],
         [2, 0, 1],
         [0, 1, 2]])


🔹 2️ largest=False (오름차순 정렬)


sorted_values_asc, sorted_indices_asc = torch.topk(x, k=x.size(1), dim=1, largest=False)

print("\nSorted values (ascending order):\n", sorted_values_asc)
print("Sorted indices:\n", sorted_indices_asc)

📌 예상 출력 결과

Sorted values (ascending order):
 tensor([[1, 3, 7],
         [2, 4, 9],
         [5, 6, 8]])

Sorted indices:
 tensor([[0, 2, 1],
         [1, 0, 2],
         [2, 1, 0]])

✅ 정리
옵션	동작	결과
largest=True	큰 값부터 정렬	내림차순 (최댓값이 먼저)
largest=False	작은 값부터 정렬	오름차순 (최솟값이 먼저)
k=x.size(dim)	전체 정렬 효과	완전히 정렬된 결과

✅ torch.topk(x, k=x.size(dim), dim, largest=True/False)를 사용하면 정렬 효과를 얻을 수 있음! 🚀

=============================================================================
2-13-masked_fill() 예제

import torch

# 3x3 텐서 생성
x = torch.tensor([[1, 7, 3],
                  [4, 2, 9],
                  [8, 6, 5]])

# 마스크 생성 (True인 위치에 -1을 채울 것)
mask = x > 5  # 값이 5보다 큰 경우 True

# masked_fill 적용
x_masked = x.masked_fill(mask, value=-1)

# 결과 출력
print("Original Tensor:\n", x)
print("\nMask:\n", mask)
print("\nMasked Tensor:\n", x_masked)


📌 예상 출력 결과

Original Tensor:
 tensor([[1, 7, 3],
         [4, 2, 9],
         [8, 6, 5]])

Mask:
 tensor([[False,  True, False],
         [False, False,  True],
         [ True,  True, False]])

Masked Tensor:
 tensor([[ 1, -1,  3],
         [ 4,  2, -1],
         [-1, -1,  5]])
🔹 설명
mask = x > 5
True가 되는 위치는 x 값이 5보다 큰 경우.
x.masked_fill(mask, value=-1)
mask=True인 위치를 -1로 변경.
📌 mask 결과



tensor([[False,  True, False],
        [False, False,  True],
        [ True,  True, False]])
→ 7, 9, 8, 6이 있는 위치가 True.

📌 masked_fill() 결과



tensor([[ 1, -1,  3],
        [ 4,  2, -1],
        [-1, -1,  5]])
→ 7, 9, 8, 6을 -1로 변경.

✅ 정리

함수	기능
x.masked_fill(mask, value=-1)	mask=True인 위치를 -1로 변경
mask = x > 5	5보다 큰 값의 위치를 True로 설정
mask = x == 3	특정 값(예: 3)의 위치를 True로 설정

✅ masked_fill()을 사용하면 특정 조건을 만족하는 값을 쉽게 변경할 수 있음!

=============================================================================
2-14-ones(), zeros(), ones_like(), zeros_like() 예제



import torch

# 1️ ones() - 3x3 크기의 1로 채워진 텐서 생성
ones_tensor = torch.ones(3, 3)

# 2️ zeros() - 3x3 크기의 0으로 채워진 텐서 생성
zeros_tensor = torch.zeros(3, 3)

# 3️ ones_like() - 기존 텐서와 같은 크기의 1로 채워진 텐서 생성
x = torch.tensor([[2, 3, 4], [5, 6, 7]])  # (2,3) 크기 텐서
ones_like_tensor = torch.ones_like(x)

# 4️ zeros_like() - 기존 텐서와 같은 크기의 0으로 채워진 텐서 생성
zeros_like_tensor = torch.zeros_like(x)

# 결과 출력
print("ones():\n", ones_tensor)
print("\nzeros():\n", zeros_tensor)
print("\nones_like(x):\n", ones_like_tensor)
print("\nzeros_like(x):\n", zeros_like_tensor)

📌 예상 출력 결과



ones():
 tensor([[1., 1., 1.],
         [1., 1., 1.],
         [1., 1., 1.]])

zeros():
 tensor([[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]])

ones_like(x):
 tensor([[1, 1, 1],
         [1, 1, 1]])

zeros_like(x):
 tensor([[0, 0, 0],
         [0, 0, 0]])

🔹 설명
✅ torch.ones(size)
지정한 size만큼 1로 채워진 텐서 생성.
예: torch.ones(3,3) → (3,3) 크기의 1로 채워진 텐서.

✅ torch.zeros(size)
지정한 size만큼 0으로 채워진 텐서 생성.
예: torch.zeros(3,3) → (3,3) 크기의 0으로 채워진 텐서.

✅ torch.ones_like(tensor)
주어진 tensor와 같은 크기의 1로 채워진 텐서 생성.
예: torch.ones_like(x) → x와 같은 크기의 1로 채워진 텐서.

✅ torch.zeros_like(tensor)
주어진 tensor와 같은 크기의 0으로 채워진 텐서 생성.
예: torch.zeros_like(x) → x와 같은 크기의 0으로 채워진 텐서.

✅ 정리
함수	기능	예제 결과 ((2,3))
torch.ones(2,3)	(2,3) 크기의 1로 채운 텐서 생성	[[1,1,1], [1,1,1]]
torch.zeros(2,3)	(2,3) 크기의 0으로 채운 텐서 생성	[[0,0,0], [0,0,0]]
torch.ones_like(x)	x와 같은 크기의 1로 채운 텐서 생성	[[1,1,1], [1,1,1]]
torch.zeros_like(x)	x와 같은 크기의 0으로 채운 텐서 생성	[[0,0,0], [0,0,0]]

=============================================================================





























