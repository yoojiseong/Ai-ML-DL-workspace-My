위 그래프는 대표적인 활성화 함수들의 형태를 보여줍니다.

ReLU (Rectified Linear Unit)

음수 값은 0으로 변환하고, 양수 값은 그대로 유지합니다.
비선형성을 추가하면서도 계산이 효율적이고, 기울기 소실(Vanishing Gradient) 문제를 완화합니다.

Sigmoid

출력이 0과 1 사이의 값으로 제한되며, 이진 분류(Binary Classification)에 사용됩니다.
그러나 큰 값에서 기울기가 매우 작아지는 기울기 소실 문제가 발생할 수 있습니다.

Tanh (Hyperbolic Tangent)

출력이 -1과 1 사이의 값으로 제한되며, Sigmoid보다 중심이 0에 가까워 학습이 더 빠르게 진행될 수 있습니다.
하지만 극단적인 값에서 여전히 기울기 소실 문제가 발생할 수 있습니다.

Softmax

다중 클래스 분류(Multi-class Classification)에서 사용되며, 입력값을 확률 분포 형태로 변환합니다.
각 클래스에 대한 확률을 출력하며, 총합이 1이 됩니다.
각 함수가 딥러닝에서 어떻게 사용되는지 추가적으로 궁금한 점이 있으면 질문해주세요! 🚀 ​

📝 1. 기본 개념 및 설명

정의: 신경망(딥러닝)의 각 뉴런(노드)에서 입력 신호의 총합을 다음 뉴런으로 전달할지 말지, 또는 얼마나 강하게 전달할지를 결정하는 비선형 함수입니다.
 일종의 '판단 기준' 또는 '게이트' 역할을 합니다.
왜 사용할까? (목표):
비선형성 도입: 가장 중요한 이유입니다. 활성화 함수가 없다면 신경망은 아무리 깊어져도 단순히 선형 함수(직선)의 연속일 뿐입니다.
복잡한 비선형 관계를 학습할 수 없게 되어 선형 회귀와 다를 바 없어집니다.
활성화 함수는 신경망이 복잡하고 비선형적인 패턴(예: 이미지의 곡선, 음성의 미묘한 변화)을 학습할 수 있게 합니다.
출력 값의 스케일 조절: 뉴런의 출력을 특정 범위(예: 0~1 또는 -1~1)로 제한하여 안정적인 학습을 돕습니다.
정보 필터링: 중요한 신호는 통과시키고, 중요하지 않거나 미약한 신호는 약화시키거나 걸러내는 역할을 합니다.
작동 방식:
각 뉴런은 이전 층의 뉴런들로부터 여러 입력 신호(x1,x2,…)를 받아 가중치(w1,w2,…)를 곱하고, 편향(bias, b)을 더하여 총합(Z)을 계산합니다.
Z=(x1⋅w1)+(x2⋅w2)+⋯+b
이렇게 계산된 총합(Z)이 바로 활성화 함수의 입력으로 들어갑니다.
활성화 함수는 이 Z 값을 받아서 최종 출력 값(A)을 계산하고, 이 A 값을 다음 층의 뉴런으로 전달합니다.
A=활성화함수(Z)

💡 2. 쉬운 예시: "ON/OFF 스위치"와 "강도 조절기"

우리 뇌의 뉴런이 어떤 자극을 받을 때 일정 수준 이상의 자극이 와야만 다음 뉴런으로 신호를 전달하듯이, 인공 신경망의 활성화 함수도 비슷한 역할을 합니다.
예시 1: 단순한 ON/OFF 스위치 (계단 함수 - 과거 사용)
상황: "이메일이 스팸일까 아닐까?"를 판단하는 아주 단순한 뉴런이 있다고 가정해 봅시다.
활성화 함수: 만약 총 입력 신호가 0.5보다 크면 1 (스팸), 아니면 0 (스팸 아님)을 출력
결과:
입력 신호 총합이 0.3이면 활성화 함수는 0을 출력 (다음 뉴런으로 "스팸 아님" 전달)
입력 신호 총합이 0.7이면 활성화 함수는 1을 출력 (다음 뉴런으로 "스팸" 전달)
문제점: 이렇게 갑자기 0에서 1로 변하는 함수는 미분하기 어려워 딥러닝 학습에 적합하지 않습니다.

예시 2: 강도 조절이 가능한 스위치 (시그모이드, ReLU 등 - 현재 사용)
상황: "이 강아지 사진이 얼마나 강아지일까?" (강아지일 확률)를 판단하는 뉴런.
활성화 함수: 입력 신호의 총합을 받아 0과 1 사이의 부드러운 확률 값으로 변환하여 출력 (예: 시그모이드 함수) 또는 음수는 0으로, 양수는 그대로 통과 (예: ReLU 함수)
결과 (시그모이드 함수 예시):
입력 신호 총합이 -2이면 활성화 함수는 약 0.12를 출력 (강아지일 확률이 낮음)
입력 신호 총합이 0이면 활성화 함수는 0.5를 출력 (애매함)
입력 신호 총합이 2이면 활성화 함수는 약 0.88을 출력 (강아지일 확률이 높음)
이처럼 활성화 함수는 단순히 ON/OFF를 넘어, 신호의 강도를 조절하여 다음 층으로 전달함으로써 신경망이 복잡한 패턴과 관계를 섬세하게 학습할 수 있도록 만드는 핵심적인 역할을 합니다. 활성화 함수 없이는 딥러닝은 비선형 문제를 전혀 풀 수 없게 됩니다.